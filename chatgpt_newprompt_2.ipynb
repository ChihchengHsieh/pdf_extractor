{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef04d696",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Processing [1] #1 - Ahn 2020.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mike8\\AppData\\Local\\Temp\\ipykernel_697068\\4226592700.py:119: DeprecationWarning: The Assistants API is deprecated in favor of the Responses API\n",
      "  thread = client.beta.threads.create()\n",
      "C:\\Users\\mike8\\AppData\\Local\\Temp\\ipykernel_697068\\4226592700.py:89: DeprecationWarning: The Assistants API is deprecated in favor of the Responses API\n",
      "  return func(*args, **kwargs)\n",
      "C:\\Users\\mike8\\AppData\\Local\\Temp\\ipykernel_697068\\4226592700.py:143: DeprecationWarning: The Assistants API is deprecated in favor of the Responses API\n",
      "  run_status = client.beta.threads.runs.retrieve(\n",
      "C:\\Users\\mike8\\AppData\\Local\\Temp\\ipykernel_697068\\4226592700.py:158: DeprecationWarning: The Assistants API is deprecated in favor of the Responses API\n",
      "  messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
      "Processing PDFs:  33%|███▎      | 1/3 [01:53<03:46, 113.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Processing [2] #3 - Aoki 2022.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  67%|██████▋   | 2/3 [03:51<01:56, 116.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Processing [3] #4 - AuYeung 2023.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|██████████| 3/3 [05:48<00:00, 116.07s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import openai\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from secret import OPENAI_API_KEY\n",
    "\n",
    "import regex  # instead of re\n",
    "\n",
    "REQUESTING_PROMPT = \"\"\"Please extract answers for A1–A32 and B1–B24 from the uploaded MR study, using the definitions in RubricQ.pdf.\n",
    "Output format:\n",
    "Provide exactly two rows only inside a Markdown code block (triple backticks) so it appears as a copy‑pasteable grey‑cell table.\n",
    "Row 1: Tab‑delimited answers only (Yes / No / Partial / Not applicable). The first field must be the StudyID in the format AuthorYear (retain all characters in the author’s name exactly as written in the paper). Followed by A1→A32 then B1→B24 (exactly 57 fields separated by 56 tab characters).\n",
    "Row 2: Tab‑delimited reasons only. The first field must again be the same StudyID. Each reason must be one short quoted phrase (no line breaks). If information is missing, write “Not reported”. DO NOT leave any field blank. Must contain exactly 57 fields separated by 56 tab characters, aligned one-to-one with Row 1.\n",
    "Rules:\n",
    "- Do not include headers, numbering, bullet points, or extra text.\n",
    "- Clearly separate Row 1 and Row 2 with one blank line. \n",
    "- Before returning, validate: Count = 57 fields for each row (if not, auto‑fill “Not reported” until 57).\n",
    "\"\"\"\n",
    "\n",
    "def extract_json_objects(text):\n",
    "    # Try to load full string as a list of dicts\n",
    "    try:\n",
    "        data = json.loads(text)\n",
    "        if isinstance(data, list):\n",
    "            return data[0]\n",
    "        else:\n",
    "            return data\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Response:\")  # Print first 500 chars for debugging\n",
    "        print(text[:500])\n",
    "        pass  # Try regex fallback below\n",
    "\n",
    "    # Fallback regex in case it's not clean JSON\n",
    "    pattern = r\"\\{(?:[^{}]|(?R))*\\}\"  # recursively match nested {}\n",
    "    matches = regex.findall(pattern, text)\n",
    "\n",
    "    objects = []\n",
    "    for m in matches:\n",
    "        try:\n",
    "            obj = json.loads(m)\n",
    "            objects.append(obj)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "    return objects[0] if objects else None\n",
    "\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = OPENAI_API_KEY  # Ensure this environment variable is set\n",
    "\n",
    "# Paths\n",
    "pdf_dir = \"./documents\"\n",
    "output_csv = \"chatgpt_new_extracted_mr_data.csv\"\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI(\n",
    "    api_key=openai.api_key,\n",
    ")\n",
    "\n",
    "# Upload the Word file (rubric)\n",
    "with open(\"./RubricQ.pdf\", \"rb\") as f:\n",
    "    rubric_file = client.files.create(file=f, purpose=\"assistants\")\n",
    "\n",
    "# Create assistant WITHOUT attaching rubric file\n",
    "assistant = client.beta.assistants.create(\n",
    "    name=\"MR Study Extractor\",\n",
    "    instructions=\"You are a genetic epidemiologist and Methods Audit reviewer. Refer to the uploaded rubric for all scoring and extraction instructions.\",\n",
    "    model=\"gpt-4-turbo\",\n",
    "    tools=[{\"type\": \"file_search\"}],\n",
    ")\n",
    "\n",
    "# Create directory for responses\n",
    "RESPONSE_DIR = \"./responses\"\n",
    "os.makedirs(RESPONSE_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "class GlobalDebug:\n",
    "    pass\n",
    "\n",
    "import random\n",
    "\n",
    "def retry_with_backoff(func, *args, **kwargs):\n",
    "    max_retries = 5\n",
    "    delay = 10  # Start with 10 seconds\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except openai.RateLimitError as e:\n",
    "            wait_time = getattr(e, \"retry_after\", delay)\n",
    "            print(f\"⚠️ Rate limit hit: retrying in {wait_time:.2f}s...\")\n",
    "            time.sleep(wait_time + random.uniform(0, 2))  # jitter\n",
    "            delay = min(delay * 2, 60)  # exponential backoff, max 60s\n",
    "        except openai.APIError as e:\n",
    "            print(f\"⚠️ API error: {e}. Retrying in {delay}s...\")\n",
    "            time.sleep(delay + random.uniform(0, 2))\n",
    "            delay = min(delay * 2, 60)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Unexpected error: {e}\")\n",
    "            raise\n",
    "    raise RuntimeError(\"❌ Max retries exceeded.\")\n",
    "\n",
    "\n",
    "def process_pdf(file_path, index):\n",
    "    file_name = os.path.basename(file_path)\n",
    "    print(f\"\\n🔍 Processing [{index}] {file_name}\")\n",
    "\n",
    "    # Upload the PDF\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            uploaded_pdf = client.files.create(file=f, purpose=\"assistants\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR][{index}] Failed to upload file {file_name}:\\n{e}\")\n",
    "        return None\n",
    "\n",
    "    # Create a thread\n",
    "    try:\n",
    "        thread = client.beta.threads.create()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR][{index}] Failed to create thread for {file_name}:\\n{e}\")\n",
    "        return None\n",
    "\n",
    "    def run_prompt(prompt, label):\n",
    "        retry_with_backoff(\n",
    "            client.beta.threads.messages.create,\n",
    "            thread_id=thread.id,\n",
    "            role=\"user\",\n",
    "            content=prompt,\n",
    "            attachments=[\n",
    "                {\"file_id\": uploaded_pdf.id, \"tools\": [{\"type\": \"file_search\"}]},\n",
    "                {\"file_id\": rubric_file.id, \"tools\": [{\"type\": \"file_search\"}]},\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        run = retry_with_backoff(\n",
    "            client.beta.threads.runs.create,\n",
    "            thread_id=thread.id,\n",
    "            assistant_id=assistant.id,\n",
    "        )\n",
    "\n",
    "        while True:\n",
    "            run_status = client.beta.threads.runs.retrieve(\n",
    "                thread_id=thread.id, run_id=run.id\n",
    "            )\n",
    "\n",
    "            GlobalDebug.run_status = run_status\n",
    "            GlobalDebug.thread_id = thread.id\n",
    "            GlobalDebug.run_id = run.id\n",
    "\n",
    "            if run_status.status == \"completed\":\n",
    "                break\n",
    "            elif run_status.status == \"failed\":\n",
    "                print(f\"Status info: {run_status}\")\n",
    "                raise RuntimeError(f\"Run failed on {label}\")\n",
    "            time.sleep(1)\n",
    "\n",
    "        messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
    "        response_text = messages.data[0].content[0].text.value\n",
    "\n",
    "        # Save raw response\n",
    "        raw_path = os.path.join(RESPONSE_DIR, f\"{index:03d}_{file_name}_{label}.txt\")\n",
    "        with open(raw_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(response_text)\n",
    "\n",
    "        return response_text\n",
    "\n",
    "    # Run prompt and get data\n",
    "    response_text = run_prompt(\n",
    "        REQUESTING_PROMPT,\n",
    "        \"scoring\",\n",
    "    )\n",
    "\n",
    "    if not response_text:\n",
    "        print(f\"[SKIP][{index}] Skipping {file_name} due to failed prompt.\")\n",
    "        return None\n",
    "\n",
    "    combined = {\"study_id\": file_name, \"response_text\": response_text}\n",
    "    return combined\n",
    "\n",
    "\n",
    "# Process all PDFs in the directory\n",
    "results = []\n",
    "pdf_files = sorted([f for f in os.listdir(pdf_dir) if f.endswith(\".pdf\")])\n",
    "\n",
    "for i, pdf_file in enumerate(tqdm(pdf_files, desc=\"Processing PDFs\"), 1):\n",
    "    pdf_path = os.path.join(pdf_dir, pdf_file)\n",
    "    result = process_pdf(pdf_path, i)\n",
    "    if result:\n",
    "        results.append(result)\n",
    "    time.sleep(90)\n",
    "\n",
    "\n",
    "# create a file and save results in json\n",
    "with open(\"extracted_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7afe36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
