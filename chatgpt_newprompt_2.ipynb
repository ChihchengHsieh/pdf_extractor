{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef04d696",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Processing [1] #1 - Ahn 2020.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mike8\\AppData\\Local\\Temp\\ipykernel_697068\\4226592700.py:119: DeprecationWarning: The Assistants API is deprecated in favor of the Responses API\n",
      "  thread = client.beta.threads.create()\n",
      "C:\\Users\\mike8\\AppData\\Local\\Temp\\ipykernel_697068\\4226592700.py:89: DeprecationWarning: The Assistants API is deprecated in favor of the Responses API\n",
      "  return func(*args, **kwargs)\n",
      "C:\\Users\\mike8\\AppData\\Local\\Temp\\ipykernel_697068\\4226592700.py:143: DeprecationWarning: The Assistants API is deprecated in favor of the Responses API\n",
      "  run_status = client.beta.threads.runs.retrieve(\n",
      "C:\\Users\\mike8\\AppData\\Local\\Temp\\ipykernel_697068\\4226592700.py:158: DeprecationWarning: The Assistants API is deprecated in favor of the Responses API\n",
      "  messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
      "Processing PDFs:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [01:53<03:46, 113.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Processing [2] #3 - Aoki 2022.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [03:51<01:56, 116.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Processing [3] #4 - AuYeung 2023.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [05:48<00:00, 116.07s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import openai\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from secret import OPENAI_API_KEY\n",
    "\n",
    "import regex  # instead of re\n",
    "\n",
    "REQUESTING_PROMPT = \"\"\"Please extract answers for A1â€“A32 and B1â€“B24 from the uploaded MR study, using the definitions in RubricQ.pdf.\n",
    "Output format:\n",
    "Provide exactly two rows only inside a Markdown code block (triple backticks) so it appears as a copyâ€‘pasteable greyâ€‘cell table.\n",
    "Rowâ€¯1: Tabâ€‘delimited answers only (Yes / No / Partial / Not applicable). The first field must be the StudyID in the format AuthorYear (retain all characters in the authorâ€™s name exactly as written in the paper). Followed by A1â†’A32 then B1â†’B24 (exactly 57 fields separated by 56 tab characters).\n",
    "Rowâ€¯2: Tabâ€‘delimited reasons only. The first field must again be the same StudyID. Each reason must be one short quoted phrase (no line breaks). If information is missing, write â€œNot reportedâ€. DO NOT leave any field blank. Must contain exactly 57 fields separated by 56 tab characters, aligned one-to-one with Row 1.\n",
    "Rules:\n",
    "- Do not include headers, numbering, bullet points, or extra text.\n",
    "- Clearly separate Rowâ€¯1 and Rowâ€¯2 with one blank line. \n",
    "- Before returning, validate: Count = 57 fields for each row (if not, autoâ€‘fill â€œNot reportedâ€ until 57).\n",
    "\"\"\"\n",
    "\n",
    "def extract_json_objects(text):\n",
    "    # Try to load full string as a list of dicts\n",
    "    try:\n",
    "        data = json.loads(text)\n",
    "        if isinstance(data, list):\n",
    "            return data[0]\n",
    "        else:\n",
    "            return data\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Response:\")  # Print first 500 chars for debugging\n",
    "        print(text[:500])\n",
    "        pass  # Try regex fallback below\n",
    "\n",
    "    # Fallback regex in case it's not clean JSON\n",
    "    pattern = r\"\\{(?:[^{}]|(?R))*\\}\"  # recursively match nested {}\n",
    "    matches = regex.findall(pattern, text)\n",
    "\n",
    "    objects = []\n",
    "    for m in matches:\n",
    "        try:\n",
    "            obj = json.loads(m)\n",
    "            objects.append(obj)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "    return objects[0] if objects else None\n",
    "\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = OPENAI_API_KEY  # Ensure this environment variable is set\n",
    "\n",
    "# Paths\n",
    "pdf_dir = \"./documents\"\n",
    "output_csv = \"chatgpt_new_extracted_mr_data.csv\"\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = openai.OpenAI(\n",
    "    api_key=openai.api_key,\n",
    ")\n",
    "\n",
    "# Upload the Word file (rubric)\n",
    "with open(\"./RubricQ.pdf\", \"rb\") as f:\n",
    "    rubric_file = client.files.create(file=f, purpose=\"assistants\")\n",
    "\n",
    "# Create assistant WITHOUT attaching rubric file\n",
    "assistant = client.beta.assistants.create(\n",
    "    name=\"MR Study Extractor\",\n",
    "    instructions=\"You are a genetic epidemiologist and Methods Audit reviewer. Refer to the uploaded rubric for all scoring and extraction instructions.\",\n",
    "    model=\"gpt-4-turbo\",\n",
    "    tools=[{\"type\": \"file_search\"}],\n",
    ")\n",
    "\n",
    "# Create directory for responses\n",
    "RESPONSE_DIR = \"./responses\"\n",
    "os.makedirs(RESPONSE_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "class GlobalDebug:\n",
    "    pass\n",
    "\n",
    "import random\n",
    "\n",
    "def retry_with_backoff(func, *args, **kwargs):\n",
    "    max_retries = 5\n",
    "    delay = 10  # Start with 10 seconds\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except openai.RateLimitError as e:\n",
    "            wait_time = getattr(e, \"retry_after\", delay)\n",
    "            print(f\"âš ï¸ Rate limit hit: retrying in {wait_time:.2f}s...\")\n",
    "            time.sleep(wait_time + random.uniform(0, 2))  # jitter\n",
    "            delay = min(delay * 2, 60)  # exponential backoff, max 60s\n",
    "        except openai.APIError as e:\n",
    "            print(f\"âš ï¸ API error: {e}. Retrying in {delay}s...\")\n",
    "            time.sleep(delay + random.uniform(0, 2))\n",
    "            delay = min(delay * 2, 60)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Unexpected error: {e}\")\n",
    "            raise\n",
    "    raise RuntimeError(\"âŒ Max retries exceeded.\")\n",
    "\n",
    "\n",
    "def process_pdf(file_path, index):\n",
    "    file_name = os.path.basename(file_path)\n",
    "    print(f\"\\nðŸ” Processing [{index}] {file_name}\")\n",
    "\n",
    "    # Upload the PDF\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            uploaded_pdf = client.files.create(file=f, purpose=\"assistants\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR][{index}] Failed to upload file {file_name}:\\n{e}\")\n",
    "        return None\n",
    "\n",
    "    # Create a thread\n",
    "    try:\n",
    "        thread = client.beta.threads.create()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR][{index}] Failed to create thread for {file_name}:\\n{e}\")\n",
    "        return None\n",
    "\n",
    "    def run_prompt(prompt, label):\n",
    "        retry_with_backoff(\n",
    "            client.beta.threads.messages.create,\n",
    "            thread_id=thread.id,\n",
    "            role=\"user\",\n",
    "            content=prompt,\n",
    "            attachments=[\n",
    "                {\"file_id\": uploaded_pdf.id, \"tools\": [{\"type\": \"file_search\"}]},\n",
    "                {\"file_id\": rubric_file.id, \"tools\": [{\"type\": \"file_search\"}]},\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        run = retry_with_backoff(\n",
    "            client.beta.threads.runs.create,\n",
    "            thread_id=thread.id,\n",
    "            assistant_id=assistant.id,\n",
    "        )\n",
    "\n",
    "        while True:\n",
    "            run_status = client.beta.threads.runs.retrieve(\n",
    "                thread_id=thread.id, run_id=run.id\n",
    "            )\n",
    "\n",
    "            GlobalDebug.run_status = run_status\n",
    "            GlobalDebug.thread_id = thread.id\n",
    "            GlobalDebug.run_id = run.id\n",
    "\n",
    "            if run_status.status == \"completed\":\n",
    "                break\n",
    "            elif run_status.status == \"failed\":\n",
    "                print(f\"Status info: {run_status}\")\n",
    "                raise RuntimeError(f\"Run failed on {label}\")\n",
    "            time.sleep(1)\n",
    "\n",
    "        messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
    "        response_text = messages.data[0].content[0].text.value\n",
    "\n",
    "        # Save raw response\n",
    "        raw_path = os.path.join(RESPONSE_DIR, f\"{index:03d}_{file_name}_{label}.txt\")\n",
    "        with open(raw_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(response_text)\n",
    "\n",
    "        return response_text\n",
    "\n",
    "    # Run prompt and get data\n",
    "    response_text = run_prompt(\n",
    "        REQUESTING_PROMPT,\n",
    "        \"scoring\",\n",
    "    )\n",
    "\n",
    "    if not response_text:\n",
    "        print(f\"[SKIP][{index}] Skipping {file_name} due to failed prompt.\")\n",
    "        return None\n",
    "\n",
    "    combined = {\"study_id\": file_name, \"response_text\": response_text}\n",
    "    return combined\n",
    "\n",
    "\n",
    "# Process all PDFs in the directory\n",
    "results = []\n",
    "pdf_files = sorted([f for f in os.listdir(pdf_dir) if f.endswith(\".pdf\")])\n",
    "\n",
    "for i, pdf_file in enumerate(tqdm(pdf_files, desc=\"Processing PDFs\"), 1):\n",
    "    pdf_path = os.path.join(pdf_dir, pdf_file)\n",
    "    result = process_pdf(pdf_path, i)\n",
    "    if result:\n",
    "        results.append(result)\n",
    "    time.sleep(90)\n",
    "\n",
    "\n",
    "# create a file and save results in json\n",
    "with open(\"extracted_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7afe36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
